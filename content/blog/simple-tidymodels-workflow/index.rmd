---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/
title: "A Basic XGBoost Tidymodels Workflow"
subtitle: ""
summary: "We will use the diamonds dataset to perform a simple tidymodels workflow to predict the price of the diamond by feeding several physical attributes to the algorithm."
authors: ["Luis Dominguez"]
tags: ["r","EDA","tidyverse"]
categories: ["r","EDA","tidyverse"]
date: 2021-04-30T00:22:27-06:00
lastmod: 2021-04-30T00:22:27-06:00
featured: true
draft: false
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: "The Glorious Studio"
  focal_point: "Smart"
  preview_only: true
projects: []
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 6, fig.height = 6)
```

## Introduction

The goal of this post is to set the ground of a [tidymodels](https://www.tidymodels.org/) workflow. We will define the model, the preprocessing recipe, tune the model paramenters, and evaluate our predictions.we will use the [diamonds dataset](https://ggplot2.tidyverse.org/reference/diamonds.html) that comes with [ggplot2](https://ggplot2.tidyverse.org/index.html). We will predict diamonds price based on their cut, colour, clarity & other attributes and it also covers the building a simple linear regression model using [tidymodels](https://www.tidymodels.org/).

```{r include=FALSE}

library(tidyverse)
library(lubridate)

theme_set(theme_bw() + theme(legend.position = "none"))

```

## The Data

```{r}
head(diamonds)
```

The goal of this post is not performing an eda of this dataset, so I will not spend too long exploring the data. I will include a quick glimpse for context.

```{r}
library(skimr)

skim(diamonds)
```

Anyone who has seen any heist movies knows that carat and price are highly correlated. A carat is the unit of weight for precious stones and pearls, now equivalent to 200 milligrams.

We will also check for correlation with other features.

```{r}
diamonds %>% 
  select_if(is.numeric) %>% 
  cor() %>% 
  as_tibble(rownames = "variable1") %>% 
  pivot_longer(cols = -variable1,
               names_to = "variable2") %>% 
  filter(variable1 != variable2) %>%
  ggplot(aes(variable1,variable2, label = round(value,2),fill = value)) +
  geom_raster() +
  geom_text() +
  scale_fill_gradient2()

```

The data is mostly clean, but I believe it will need some basic preprocessing.

## Model

### Dataset Splits and Folds

```{r}
library(tidymodels)

set.seed(2345)
split <- initial_split(data = diamonds, strata = price)
train <- training(split)
test <- testing(split)
```

```{r}
set.seed(123)
k_fold <- vfold_cv(data = train, v = 5, strata = price)

```

### Preprocessing

We don't need much transformation in the `diamonds` dataset but we do need to transform our nominal (factors) predictors into dummy variables.

```{r}
library(themis)

xgb_recipe <- recipe(price ~., data = train) %>% 
  step_dummy(all_nominal())
  

```

### Set Model Parameters

There are a fair amount of parameters in a boosted tree. You can look at the [general boosted tree parameters](https://parsnip.tidymodels.org/reference/boost_tree.html) to learn more about what they do.For now, we will focus on how to optimize the value of these parameters using the function `tune()`.

```{r}
xgb_specs <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), 
  min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune()                          
) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")
```

Now that the parameters are set to `tune()`. We need to construct a grid with parameter values that they algorithm will try and evaluate what combination is the best. I like to use `grid_max_entropy()` because it supposed to give a good coverage. You can learn more about the different grid construction methods in the [dials](https://dials.tidymodels.org/) package documentation.

```{r}
xgb_grid <- grid_max_entropy(
  tree_depth(), 
  min_n(), 
  loss_reduction(),                     
  sample_prop() ,
  finalize(mtry(), train),         
  learn_rate(),
  size = 10
  )
```

### Tuning Workflow

[Yardstick metrics](https://yardstick.tidymodels.org/articles/metric-types.html)

```{r}
xgb_wf <- workflow() %>% 
  add_model(xgb_specs) %>% 
  add_recipe(xgb_recipe)

```

The next step is using our grid and resamples to tune the parameters through cross validation. To save building time, I ran the tuning locally and saved the results.

```{r}
# doParallel::registerDoParallel()
# 
# set.seed(896)
# tuning <- tune_grid(
#   xgb_wf,
#   resamples = k_fold,
#   grid = xgb_grid,
#   metrics = metric_set(rmse,mae,mape),
#   control = control_grid(verbose = TRUE)
#   
# )
# 
# save(tuning, file = "~/Documents/GitHub/personal_website/content/blog/simple-tidymodels-workflow/tuning.Rdata")

load("~/Documents/GitHub/personal_website/content/blog/simple-tidymodels-workflow/tuning.Rdata")

tuning

```

The `metric_set()` function sets the chosen metrics to use in the selection process. Something to notice is that in classification problems you may need to tell the function what event should be considered as the "success" in `metric_set(..., event_level = "second")`, for example.

```{r}
show_best(tuning, metric = "mape")

best_params <- select_best(tuning, metric = "mape")
```

## Evaluation

```{r}
xgb_final_wf <- xgb_wf %>% 
  finalize_workflow(best_params)
```

```{r}
xgb_final_fit <- last_fit(xgb_final_wf, 
                          split = split,
                          metrics = metric_set(rmse,mae,mape))

collect_metrics(xgb_final_fit)
```

It seems we didn't overfit our model.

```{r}
pred <- collect_predictions(xgb_final_fit)

pred %>% 
  ggplot(aes(price,.pred)) +
  geom_point(size = 0.3, color = "midnightblue",alpha = 0.6) +
  geom_abline(slope = 1) +
  coord_fixed() +
  labs(x = "Price",y = "Prediction")
```

Our model does worse with expensive diamonds. It would be worth it to remove outliers before training. Let's look at clarity and how the model perform in the different clarity categories.

```{r}

pred %>% 
  bind_cols(test %>% select(-price)) %>%
  mutate(perror = abs(.pred - price)/price) %>% 
  ggplot(aes(clarity,perror, fill = clarity)) +
  geom_boxplot(alpha = 0.3) +
  scale_y_sqrt() + 
  labs(x = "Clarity", y = "Percent Error")

```

Our model struggles with I1 clarity but besides this, I think this was a good attempt on applying the [tidymodels](https://www.tidymodels.org/) approach.

[Photo credit](https://www.pexels.com/@the-glorious-studio-3584518?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)
